{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of char_recognition_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkH+ZzU5jumEiPXJxcGLjO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajjawal-Prayank/ML-Projects/blob/master/character_recognition_cnn_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzfy6lqOIDYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "16207802-7ef7-4313-ebdb-76101d6de876"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-YSae0qjCeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaT18m5UPL5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b637f3c9-6e10-4566-b373-62f903ae7371"
      },
      "source": [
        "#mounting google drive on google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG-U3FEVlpP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54489318-8347-4680-a4b4-58d88d552e3e"
      },
      "source": [
        "cd /content/gdrive/My Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceWuZfmMiKTt",
        "colab_type": "text"
      },
      "source": [
        "The EMNIST Python Package provides functionality to automatically download the dataset, and to load it as numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj-h_iwmDGsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e5490295-65f5-4ac1-888c-bebe91d4567b"
      },
      "source": [
        "pip install emnist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emnist\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/f4/78b24acbef9e8fe976dda700f16a3606f3b8363b015bc555f8050fbbd8ac/emnist-0.0-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from emnist) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from emnist) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from emnist) (1.18.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->emnist) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->emnist) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->emnist) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->emnist) (1.24.3)\n",
            "Installing collected packages: emnist\n",
            "Successfully installed emnist-0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ri8GQavEacZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3897747-be7f-4893-a7cc-fa489772acfe"
      },
      "source": [
        "from emnist import extract_training_samples\n",
        "(training_images,training_labels) = extract_training_samples('balanced')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading emnist.zip: 536MB [00:03, 158MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma10GZKIE1Ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from emnist import extract_test_samples\n",
        "testing_images,testing_labels = extract_test_samples('balanced')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4T4vfJGgHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_images_np=training_images.reshape(-1,28*28)       #converting 2d image into single dimension for calculation  \n",
        "testing_images_np=testing_images.reshape(-1,28*28)\n",
        "training_labels_one  = training_labels.reshape(-1,1)       #ensures each row has only one column\n",
        "testing_labels_one = testing_labels.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTSXlTmGN5fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRYugwtBF6XM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkSKtHGy6vjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For the ease of calculation , we keep the data values (here, value of each pixel) between certain limits (here, zero and one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGmknv2k6793",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler_model = MinMaxScaler()                                          #this makes scaler_model (a variable) an instance of  MinMaxScaler\n",
        "scaler_model.fit(training_images_np)                                   #this fits the data in the model\n",
        "train_images = scaler_model.transform(training_images_np)              #finally this converts the data between the given range.By default, 0 to 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCHvOPSQ7Af6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler_model.fit(testing_images_np)\n",
        "test_images = scaler_model.transform(testing_images_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh8iuAX77BH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48f10344-436f-40f7-b943-159971a74b77"
      },
      "source": [
        "train_images.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz6I4kMw7Bt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying one hot encoding on the labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W_T15oUwaNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoding_model = OneHotEncoder()\n",
        "\n",
        "encoding_model.fit(training_labels_one)\n",
        "train_labels = encoding_model.transform(training_labels_one).toarray()  \n",
        "#transform() return data type \"scipy.sparse.csr.csr_matrix\" . However, we need to keep our dataset of type \"numpy.ndarray\"\n",
        "#So, we use .toarray() on the transformed dataset\n",
        "\n",
        "encoding_model.fit(testing_labels_one)\n",
        "test_labels = encoding_model.transform(testing_labels_one).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1scNurg0xRea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#naming the labels\n",
        "emnist_mapping ={0:0,1:1,2:2,3:3,4:4,5:5,6:6,7:7,8:8,9:9,10:'A',11:'B',12:'C',13:'D',14:'E',\n",
        "              15:'F',16:'G',17:'H',18:'I',19:'J',20:'K',21:'l',22:'M',23:'N',24:'O',25:'P',\n",
        "              26:'Q',27:'R',28:'S',29:'T',30:'u',31:'V',32:'W',33:'X',34:'Y',35:'Z',36:'a',\n",
        "              37:'b',38:'d',39:'e',40:'f',41:'g',42:'h',43:'n',44:'q',45:'r',46:'t'}\n",
        "\n",
        "#this mapping is according to \"emnist-balanced\" dataset.\n",
        "# the following characters are left out due to similarity in their upper case and lower case representations (or similarity with other characters)\n",
        "# L, U, c,i,j,k,m,o,p,s,v,w,x,y,z  (total 15)\n",
        "#so, characters [a-z] , [A-Z], [0-9] are total 26+26+10=62\n",
        "#15 are not used. So, remaining no. of labels =62-15=47"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apQ_ZCWRWTYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m8aHjL2xuxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ylS6QcVxwCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convolutional layer\n",
        "def convolutional_layer(input_x,shape):\n",
        "    t=tf.truncated_normal(shape,stddev=0.1)\n",
        "    \"\"\" \n",
        "    tf.truncated_normal() :-\n",
        "    Outputs random values from a truncated normal distribution.\n",
        "    The generated values follow a normal distribution with specified mean and standard deviation, \n",
        "    except that values whose magnitude is more than 2 standard deviations from the mean are dropped \n",
        "    and re-picked.\n",
        "    Normal distribution, also known as the Gaussian distribution, is a probability distribution that is \n",
        "    symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far \n",
        "    from the mean. In graph form, normal distribution will appear as a bell curve.\n",
        "    \"\"\"\n",
        "    w=tf.Variable(t)        #A tf.Variable represents a tensor whose value can be changed by running ops on it.\n",
        "    b=tf.Variable(tf.constant(0.1,shape=[shape[3]]) )               # all values are 0.1 and the shape is that of the input tensor\n",
        "    return tf.nn.relu(tf.nn.conv2d(input_x,w,strides=[1,1,1,1],padding='SAME') +b)\n",
        "    # input_x is the input tensor.A 4d tensor. its shape:- [batch,height,width,channel]  (channel is 1 for grayscale and 3 for rgb)\n",
        "    # batch basically tells which all images are being taken as i/p/. height and weight are of individual images. \n",
        "    # w is the kernel(or the filter).its shape:- [filter height,filter width,no. of channels as i/p, no. of channels as o/p ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQLQ_YmLxyLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normal (or, fully connected) layer\n",
        "def normal_full_layer(input_layer,size):\n",
        "    input_size=int(input_layer.get_shape()[1])  #it is for matrix dimension. so make sure the 2 matrices multiplied are of the form (m*n) & (n*p)\n",
        "    w=tf.Variable(tf.truncated_normal([input_size,size],stddev=0.1))\n",
        "    b=tf.Variable(tf.constant(0.1,shape=[size]))\n",
        "    return tf.matmul(input_layer,w)+b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JpPG0wjxzkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaslO9hYx1Qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# placeholder\n",
        "x=tf.placeholder(tf.float32,shape=[None,784])           #input image is of dimension 28*28=784\n",
        "y_true=tf.placeholder(tf.float32,shape=[None,47])       #there are a total of 47 classes/labels/characters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-X8ioVPx1_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH8HmNiJyLuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layers\n",
        "x_image= tf.reshape(x,[-1,28,28,1])   # converting the i/p img back to layers. The 28*28 is h*w. 1 is coz of grayscale."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buW-rMWdx3Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first convolutional layer\n",
        "convo_1=convolutional_layer(x_image,shape=[5,5,1,32]) # the wt. tensor is [5,5,1,32]\n",
        "                                                      # 5 by 5 convolutional layer. so, this convolution will compute\n",
        "                                                      # 32 features for each 5 by 5 patch. 1 is the i/p channels.32 is the no.\n",
        "                                                      # of o/p channels.\n",
        "                                                      #so, 32 images(features) will be generated as the result of this \n",
        "                                                      #beacuse we basically use 32 filters of 5*5 dimension each"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdFogeY4x5IR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first pooling layer\n",
        "convo_1_pooling = tf.nn.max_pool(convo_1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
        "# convo_1 is the input tensor. its shape:- [batch,height,width,channel] \n",
        "# pooling is done only along height and width. so, it is basically 2*2 pooling\n",
        "# ksize specifies the size of the window for each dimension of input tensor\n",
        "# stride specifies the stride of the sliding window for each dimension of input tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KetjbjC2QDrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# second convolutional and pooling layer\n",
        "convo_2=convolutional_layer(convo_1_pooling,shape=[3,3,32,64])\n",
        "convo_2_pooling = tf.nn.max_pool(convo_2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bibd8sxeP6_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# third convolutional layer\n",
        "convo_3=convolutional_layer(convo_2_pooling,shape=[7,7,64,128])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiI3l3GWyRDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fourth convolutional layer\n",
        "convo_4 = convolutional_layer(convo_3,shape=[5,5,128,256])    # 256 features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOImGl3hvYkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh3ww5nBySv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this o/p is now flattened out so that it gets connected to a fully connected layer\n",
        "convo_4_flat= tf.reshape(convo_4,[-1,7*7*256])\n",
        "#we get 256 images(features) of 7*7 dimension\n",
        "#here 7*7 is dimension of each image. here's how:-\n",
        "#28*28 on pooling with 2*2 gives 14*14\n",
        "#14*14 on pooling with 2*2 gives 7*7\n",
        "#NOTE:- the dimension of the image didn't change on convolution as \"SAME\" type of convolution is used.Meaning to say that the padding is such that the final\n",
        "#       o/p dimension remains the same.here, padding=(f-1)/2=(5-1)/2=2 . so after padding , 28*28 becomes 32*32. and on convolution gives 28*28 \n",
        "#       (as, n-f+1=32-5+1=28  )\n",
        "full_layer_one=tf.nn.relu(normal_full_layer(convo_4_flat,2000)) #2000 nodes/neurons taken in first fully connected layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1rKhykRNwX0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a8917b83-91dd-4134-e8be-fb2a6645f433"
      },
      "source": [
        "# dropout\n",
        "hold_prob= tf.placeholder(tf.float32)\n",
        "full_one_dropout= tf.nn.dropout(full_layer_one,keep_prob=hold_prob) #nodes in fully connected layer are held for better results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-23-aa2771a72171>:3: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlrmpzuMu0as",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_layer_two=tf.nn.relu(normal_full_layer(full_one_dropout,1000))   #1000 nodes/neurons taken in first fully connected layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuKuysJ6u6Es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVM5nu8lyU-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dropout\n",
        "full_two_dropout= tf.nn.dropout(full_layer_two,keep_prob=hold_prob) #nodes in fully connected layer are held for better results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29lMPj8BILRc",
        "colab_type": "text"
      },
      "source": [
        "For future use:-\n",
        "WARNING:tensorflow:From <ipython-input-28-c7d08bb7c69d>:3: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
        "Instructions for updating:\n",
        "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjBtgpxxyXnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYT-trNDyaRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred= normal_full_layer(full_two_dropout,47)    # 47 is the no. of labels (so, the last layer has 47 nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyf7W1AVzp7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYMPNDZPzt0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "71dd3741-6d92-4e1e-c792-820ccd307a28"
      },
      "source": [
        "# loss function\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-27-6fcd616eba1b>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrLGkUFtKJcI",
        "colab_type": "text"
      },
      "source": [
        "For future use:-\n",
        "WARNING:tensorflow:From <ipython-input-30-6fcd616eba1b>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
        "Instructions for updating:\n",
        "\n",
        "Future major versions of TensorFlow will allow gradients to flow\n",
        "into the labels input on backprop by default.\n",
        "\n",
        "See `tf.nn.softmax_cross_entropy_with_logits_v2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B6YvLmYzuih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
        "trainer =  optimizer.minimize(cross_entropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUFuGXG0zw1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init= tf.global_variables_initializer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPePx_C9zyn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_saver = tf.train.Saver()  #used for saving the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PbsTJvAF7zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfh46T7pF9i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for data augmentation\n",
        "from scipy.ndimage.interpolation import shift\n",
        "def shift_image(image, dx, dy):\n",
        "    image = image.reshape((28, 28))\n",
        "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
        "    return shifted_image.reshape([-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1P1iP6bF9JH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "399bb5fa-6d0c-4d42-e7f1-a35121ccc3bd"
      },
      "source": [
        "start = time.time()\n",
        "epochs=20\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(epochs):\n",
        "      for j in range(0,train_images.shape[0],20):    #taking 20 images from dataset at a time\n",
        "        index_arr=np.arange(j,j+20)\n",
        "        batch_x= train_images[index_arr]\n",
        "        batch_y=train_labels[index_arr]\n",
        "\n",
        "      \n",
        "        #data augmentation\n",
        "        for dx, dy in ((2,0), (-2,0), (0,2), (0,-2)):\n",
        "          for image, label in zip(batch_x, batch_y):\n",
        "            np.append(batch_x,shift_image(image, dx, dy))\n",
        "            np.append(batch_y,label)\n",
        "          \n",
        "        \n",
        "\n",
        "        sess.run(trainer,feed_dict={x:batch_x,y_true:batch_y,hold_prob:0.6})   \n",
        "                                    # hold_prob:0.5 implies every neuron has a 50% hold probablity\n",
        "    \n",
        "      print(\"on epoch\"+str(i+1))\n",
        "      print(\"Accuracy:  \")\n",
        "      found_matches=tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
        "      #argmax(input, axis=None, name=None, dimension=None) Returns the index with the largest value across axis of a tensor\n",
        "      \n",
        "      accuracy=tf.reduce_mean(tf.cast(found_matches,tf.float32))\n",
        "      print(sess.run(accuracy,feed_dict={x:test_images,y_true:test_labels,hold_prob:1.0}))\n",
        "      # during testing we don't want to dropout any of the neurons. So, we write hold_prob:1.0 , meaning that every neuron will be held at its \n",
        "      # position and none will be dropped out\n",
        "      print(\"\\n\")\n",
        "\n",
        "      model_saver.save(sess,'my_models/model_char_recog.ckpt')\n",
        "end = time.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "on epoch1\n",
            "Accuracy:  \n",
            "0.8195745\n",
            "\n",
            "\n",
            "on epoch2\n",
            "Accuracy:  \n",
            "0.8498404\n",
            "\n",
            "\n",
            "on epoch3\n",
            "Accuracy:  \n",
            "0.8664362\n",
            "\n",
            "\n",
            "on epoch4\n",
            "Accuracy:  \n",
            "0.8656383\n",
            "\n",
            "\n",
            "on epoch5\n",
            "Accuracy:  \n",
            "0.86962765\n",
            "\n",
            "\n",
            "on epoch6\n",
            "Accuracy:  \n",
            "0.87579787\n",
            "\n",
            "\n",
            "on epoch7\n",
            "Accuracy:  \n",
            "0.8736702\n",
            "\n",
            "\n",
            "on epoch8\n",
            "Accuracy:  \n",
            "0.8762234\n",
            "\n",
            "\n",
            "on epoch9\n",
            "Accuracy:  \n",
            "0.8780319\n",
            "\n",
            "\n",
            "on epoch10\n",
            "Accuracy:  \n",
            "0.8679255\n",
            "\n",
            "\n",
            "on epoch11\n",
            "Accuracy:  \n",
            "0.87409574\n",
            "\n",
            "\n",
            "on epoch12\n",
            "Accuracy:  \n",
            "0.8745745\n",
            "\n",
            "\n",
            "on epoch13\n",
            "Accuracy:  \n",
            "0.8784574\n",
            "\n",
            "\n",
            "on epoch14\n",
            "Accuracy:  \n",
            "0.8793085\n",
            "\n",
            "\n",
            "on epoch15\n",
            "Accuracy:  \n",
            "0.8713298\n",
            "\n",
            "\n",
            "on epoch16\n",
            "Accuracy:  \n",
            "0.87590426\n",
            "\n",
            "\n",
            "on epoch17\n",
            "Accuracy:  \n",
            "0.87255317\n",
            "\n",
            "\n",
            "on epoch18\n",
            "Accuracy:  \n",
            "0.8722872\n",
            "\n",
            "\n",
            "on epoch19\n",
            "Accuracy:  \n",
            "0.87787235\n",
            "\n",
            "\n",
            "on epoch20\n",
            "Accuracy:  \n",
            "0.876383\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P074h2ATc8AC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG6KovDUCs_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predicting on single examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSqP2kPsTncQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def char_predictor(image):\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    model_saver.restore(sess, \"my_models/model_char_recog.ckpt\")\n",
        "             \n",
        "    prediction=tf.argmax(y_pred,1)\n",
        "    var = prediction.eval(feed_dict={x: [image],y_true:train_labels,hold_prob: 1.0}, session=sess)\n",
        "    print(\"The predicted character is :- \" + str(emnist_mapping[var[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvM1D2x3k6Sz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "915e1a11-09fa-4569-a25b-1d4843d183fb"
      },
      "source": [
        "#first example\n",
        "image_index = 444\n",
        "single_image = train_images[image_index].reshape(28,28) \n",
        "plt.imshow(single_image,cmap='gist_gray') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9c77c2cac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbLLmpdRxv-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3cc17e3a-7800-44e9-8f1b-5b399d6af1d9"
      },
      "source": [
        "char_predictor(train_images[image_index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from my_models/model_char_recog.ckpt\n",
            "The predicted character is :- E\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkuJsfXsTVbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxLmwHHmdeNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predicting on images given by user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeDNFSE7tim8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to convert user image into image suitable for the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCNr038Tt0i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image, ImageFilter\n",
        "\n",
        "\n",
        "def imageprepare(argv):\n",
        "    \"\"\"\n",
        "    This function returns the pixel values.\n",
        "    The imput is a image file location.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      my_image = Image.open(argv).convert('L')    #.convert() Returns a converted copy of this image. Mode L means 8-bit pixels, black and white\n",
        "    except:\n",
        "      print(\"Sorry! No such file found. Please enter full path of file along with extension\")\n",
        "      return  \n",
        "        \n",
        "    width, height = my_image.size\n",
        "    new_image = Image.new('L', (28, 28), (255))  # creates white canvas of 28x28 pixels . \n",
        "    \n",
        "\n",
        "    if width > height:  \n",
        "        #occupy all the width and change the height according to height-width ratio\n",
        "        new_height = int(round(((28.0 / width )* height),0)) \n",
        "        if (new_height == 0):  # keeping minimum  1 pixel\n",
        "            new_height = 1\n",
        "\n",
        "        # resize and sharpen\n",
        "        #Image sharpening refers to any enhancement technique that highlights edges and fine details in an image.\n",
        "        image = my_image.resize((28, new_height), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
        "        #anti-aliasing. : a procedure used in digital graphics processing for smoothing lines and removing visual distortions.\n",
        "        #The class ImageFilter.SHARPEN implements a spatial filter using convolution to sharpen a given image.\n",
        "        #To get a filter applied onto an image the filter() method is called on the Image object. Name of filter class passed to it is ImageFilter.SHARPEN\n",
        "        #The convolution matrix used is, ((-2, -2, -2),(-2, 32, -2),(-2, -2, -2)) ;a 3x3 matrix.The filter() method applies the convolution matrix to the \n",
        "        # image pixels and returns the sharpened image.\n",
        "\n",
        "        #trying to keep the image in centre of canvas\n",
        "        position = int(round(((28 - new_height) / 2), 0))  \n",
        "        new_image.paste(image, (0, position))  # paste resized image on top of white canvas\n",
        "    else:\n",
        "        #occupy all the height and change the width according to height-width ratio\n",
        "        new_width = int(round((28.0 / height * width), 0)) \n",
        "        if (new_width == 0):  \n",
        "            new_width = 1\n",
        "            \n",
        "        # resize and sharpen\n",
        "        image = my_image.resize((new_width, 28), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
        "        position = int(round(((28 - new_width) / 2), 0))  \n",
        "        new_image.paste(image, (position, 0))  # paste resized image on top of white canvas\n",
        "\n",
        "\n",
        "    lis = list(new_image.getdata())   #getdata() returns a sequence of data. The sequence object is flattened,i.e, one dimensional\n",
        "    # normalizing pixels to 0 and 1. \n",
        "    final_img = [(255 - x) * 1.0 / 255.0 for x in lis]\n",
        "    #print(tva)\n",
        "    for i in range(len(final_img)):\n",
        "        if final_img[i]<=0.45:\n",
        "            final_img[i]=0.0\n",
        "    return final_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTp5yR9rCA9n",
        "colab_type": "text"
      },
      "source": [
        "Inputs from outside of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fflZ9K_aOs9S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0c4562f8-e90d-4508-9274-35d1994dea65"
      },
      "source": [
        "#first example\n",
        "my_img=imageprepare('./P.jpg')#file path here\n",
        "my_img=np.array(my_img)\n",
        "plt.imshow(my_img.reshape(28,28),cmap='gist_gray') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff373e334e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Irp_HK0375r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7c0debb0-477e-45b1-d3ba-9e41842c586c"
      },
      "source": [
        "char_predictor(my_img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from my_models/model_char_recog.ckpt\n",
            "The predicted character is :- P\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuhQZhEyppq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZbwyr1KpqPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmCLqpbwpsvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "3b2788d6-7bdb-4eaa-8537-59229fe50338"
      },
      "source": [
        "my_img=imageprepare('./B.jpg')#file path here\n",
        "my_img=np.array(my_img)\n",
        "plt.imshow(my_img.reshape(28,28),cmap='gist_gray') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff373e12390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nl7x6Rup6uY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "158e06f6-2705-4f0d-dce3-3cf5640d44bd"
      },
      "source": [
        "char_predictor(my_img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from my_models/model_char_recog.ckpt\n",
            "The predicted character is :- B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeoUoomsCTmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfYaQ7kDqH2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#example 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L72qkuPEqJoJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "084ee88a-683c-4721-e210-2515dc67d7c1"
      },
      "source": [
        "my_img=imageprepare('./3.jpg')#file path here\n",
        "my_img=np.array(my_img)\n",
        "plt.imshow(my_img.reshape(28,28),cmap='gist_gray') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff373d6b160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yui-SZxSqKTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0c024753-e4a0-48e0-8fb7-d3d1c89e2892"
      },
      "source": [
        "char_predictor(my_img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from my_models/model_char_recog.ckpt\n",
            "The predicted character is :- 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kXWlux7qO2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmE0zQDIQZ1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#example 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZWyzQIMNq9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "eb440b94-a17d-4ca0-a3b3-95750d9c9784"
      },
      "source": [
        "my_img=imageprepare('./2.jpg')#file path here\n",
        "my_img=np.array(my_img)\n",
        "plt.imshow(my_img.reshape(28,28),cmap='gist_gray') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff373d40fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cymtzWnjQQ0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5a337db1-b562-418c-b97c-eee37b45362c"
      },
      "source": [
        "char_predictor(my_img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from my_models/model_char_recog.ckpt\n",
            "The predicted character is :- 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knkeZ0KP03yJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_my_image():\n",
        "  img=input(\"Please enter path of image file:- \\n\")\n",
        "  my_img=imageprepare(img)#file path here\n",
        "  my_img=np.array(my_img)\n",
        "  if my_img.size==1:\n",
        "    return\n",
        "  plt.imshow(my_img.reshape(28,28),cmap='gist_gray') \n",
        "  char_predictor(my_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcKZ09AWbcBm",
        "colab_type": "text"
      },
      "source": [
        "**Please Run this last cell and give in the image file to predict the character inside it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It0Oghoaahel",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "334a0846-6fc1-4f37-d344-4892ed745df4"
      },
      "source": [
        "predict_my_image()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter path of image file:- \n",
            "./character_images/D1.png\n",
            "INFO:tensorflow:Restoring parameters from my_models/model_char_recog.ckpt\n",
            "The predicted character is :- D\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
